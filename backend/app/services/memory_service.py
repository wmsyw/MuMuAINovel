"""å‘é‡è®°å¿†æœåŠ¡ - åŸºäºŽChromaDBå®žçŽ°é•¿æœŸè®°å¿†å’Œè¯­ä¹‰æ£€ç´¢"""
import chromadb
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
from app.logger import get_logger
import os
import hashlib
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.models.character import Character
from app.models.memory import StoryMemory, PlotAnalysis
from app.services.prompt_service import PromptService
from app.services.json_helper import parse_json

logger = get_logger(__name__)

# é…ç½®æ¨¡åž‹ç¼“å­˜ç›®å½•
# ä¼˜å…ˆä½¿ç”¨ backend/embedding ç›®å½•ï¼ˆæ‰“åŒ…åŽçš„å®žé™…ä½ç½®ï¼‰
import sys
from pathlib import Path

if 'SENTENCE_TRANSFORMERS_HOME' not in os.environ:
    # æ ¹æ®è¿è¡ŒçŽ¯å¢ƒç¡®å®šæ¨¡åž‹ç›®å½•
    if getattr(sys, 'frozen', False):
        # PyInstaller æ‰“åŒ…åŽ - éœ€è¦æ£€æŸ¥å¤šä¸ªå¯èƒ½çš„ä½ç½®
        exe_dir = Path(sys.executable).parent
        
        # æ£€æŸ¥é¡ºåºï¼š
        # 1. _MEIPASS/backend/embedding (ä¸´æ—¶è§£åŽ‹ç›®å½•)
        # 2. exeåŒçº§/_internal/backend/embedding
        # 3. exeåŒçº§/backend/embedding
        possible_paths = []
        
        if hasattr(sys, '_MEIPASS'):
            possible_paths.append(Path(sys._MEIPASS) / 'backend' / 'embedding')
        
        possible_paths.extend([
            exe_dir / '_internal' / 'backend' / 'embedding',
            exe_dir / 'backend' / 'embedding',
            exe_dir / '_internal' / 'embedding',
            exe_dir / 'embedding'
        ])
        
        model_dir = None
        for path in possible_paths:
            if path.exists():
                model_dir = path
                logger.info(f"ðŸ”§ æ‰¾åˆ°æ‰“åŒ…çŽ¯å¢ƒæ¨¡åž‹ç›®å½•: {model_dir}")
                break
        
        if model_dir:
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(model_dir)
        else:
            # æœ€åŽé™çº§æ–¹æ¡ˆ
            fallback_dir = exe_dir / 'embedding'
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(fallback_dir)
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°é¢„æ‰“åŒ…æ¨¡åž‹ï¼Œä½¿ç”¨é™çº§ç›®å½•: {fallback_dir}")
            logger.warning(f"   æ£€æŸ¥è¿‡çš„è·¯å¾„: {[str(p) for p in possible_paths]}")
    else:
        # å¼€å‘æ¨¡å¼ï¼Œä»Žå½“å‰æ–‡ä»¶ä½ç½®å‘ä¸Šæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
        base_dir = Path(__file__).parent.parent.parent
        model_dir = base_dir / 'backend' / 'embedding'
        if model_dir.exists():
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(model_dir)
            logger.info(f"ðŸ”§ è®¾ç½®å¼€å‘çŽ¯å¢ƒæ¨¡åž‹ç›®å½•: {model_dir}")
        else:
            # é™çº§åˆ°é¡¹ç›®æ ¹ç›®å½•çš„ embedding
            fallback_dir = base_dir / 'embedding'
            os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(fallback_dir)
            logger.info(f"ðŸ”§ ä½¿ç”¨é™çº§æ¨¡åž‹ç›®å½•: {fallback_dir}")


class MemoryService:
    """å‘é‡è®°å¿†ç®¡ç†æœåŠ¡ - å®žçŽ°è¯­ä¹‰æ£€ç´¢å’Œé•¿æœŸè®°å¿†"""
    
    _instance = None
    _initialized = False
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        """åˆå§‹åŒ–ChromaDBå’ŒEmbeddingæ¨¡åž‹"""
        if self._initialized:
            return
            
        try:
            # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
            chroma_dir = "data/chroma_db"
            os.makedirs(chroma_dir, exist_ok=True)
            
            # åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯(ä½¿ç”¨æ–°API - PersistentClient)
            self.client = chromadb.PersistentClient(path=chroma_dir)
            
            # åˆå§‹åŒ–å¤šè¯­è¨€embeddingæ¨¡åž‹(æ”¯æŒä¸­æ–‡)
            logger.info("ðŸ”„ æ­£åœ¨åŠ è½½Embeddingæ¨¡åž‹...")
            
            # ä½¿ç”¨çŽ¯å¢ƒå˜é‡ä¸­é…ç½®çš„æ¨¡åž‹ç›®å½•
            model_cache_dir = os.environ.get('SENTENCE_TRANSFORMERS_HOME', 'embedding')
            os.makedirs(model_cache_dir, exist_ok=True)
            logger.info(f"ðŸ“‚ ä½¿ç”¨æ¨¡åž‹ç¼“å­˜ç›®å½•: {os.path.abspath(model_cache_dir)}")
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°çŽ¯å¢ƒå˜é‡å’Œè·¯å¾„
            logger.info(f"ðŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
            logger.info(f"ðŸ“‚ æ¨¡åž‹ç¼“å­˜ç›®å½•: {os.path.abspath(model_cache_dir)}")
            logger.info(f"ðŸ”§ SENTENCE_TRANSFORMERS_HOME: {os.environ.get('SENTENCE_TRANSFORMERS_HOME', 'æœªè®¾ç½®')}")
            logger.info(f"ðŸ”§ TRANSFORMERS_OFFLINE: {os.environ.get('TRANSFORMERS_OFFLINE', 'æœªè®¾ç½®')}")
            logger.info(f"ðŸ”§ HF_HUB_OFFLINE: {os.environ.get('HF_HUB_OFFLINE', 'æœªè®¾ç½®')}")
            
            # æ£€æŸ¥æ¨¡åž‹ç›®å½•å†…å®¹
            abs_cache_dir = os.path.abspath(model_cache_dir)
            logger.info(f"ðŸ“‚ æ£€æŸ¥æ¨¡åž‹ç¼“å­˜ç›®å½•: {abs_cache_dir}")
            
            if os.path.exists(abs_cache_dir):
                logger.info(f"ðŸ“ æ¨¡åž‹ç›®å½•å­˜åœ¨ï¼Œæ£€æŸ¥å†…å®¹...")
                try:
                    items = os.listdir(abs_cache_dir)
                    logger.info(f"ðŸ“ æ¨¡åž‹ç›®å½•å†…å®¹ ({len(items)} é¡¹): {items}")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æœŸçš„æ¨¡åž‹æ–‡ä»¶å¤¹
                    expected_model_dir = os.path.join(abs_cache_dir, 'models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2')
                    logger.info(f"ðŸ” æ£€æŸ¥é¢„æœŸè·¯å¾„: {expected_model_dir}")
                    
                    if os.path.exists(expected_model_dir):
                        logger.info(f"âœ… æ‰¾åˆ°æœ¬åœ°æ¨¡åž‹ç›®å½•!")
                        # æ£€æŸ¥å¿«ç…§ç›®å½•
                        snapshots_dir = os.path.join(expected_model_dir, 'snapshots')
                        if os.path.exists(snapshots_dir):
                            snapshots = os.listdir(snapshots_dir)
                            logger.info(f"ðŸ“ æ¨¡åž‹å¿«ç…§ ({len(snapshots)} ä¸ª): {snapshots}")
                            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„å¿«ç…§
                            if snapshots:
                                logger.info(f"âœ… å‘çŽ°æœ‰æ•ˆå¿«ç…§ï¼Œå¯ä»¥ä½¿ç”¨ç¦»çº¿æ¨¡å¼")
                    else:
                        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡åž‹ç›®å½•")
                        logger.warning(f"   é¢„æœŸä½ç½®: {expected_model_dir}")
                except Exception as e:
                    logger.error(f"âŒ æ£€æŸ¥æ¨¡åž‹ç›®å½•å¤±è´¥: {str(e)}")
                    import traceback
                    logger.error(f"   å †æ ˆ: {traceback.format_exc()}")
            else:
                logger.warning(f"âš ï¸ æ¨¡åž‹ç›®å½•ä¸å­˜åœ¨: {abs_cache_dir}")
            
            try:
                logger.info("ðŸ”„ å°è¯•åŠ è½½ä¸»æ¨¡åž‹: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
                
                # ä½¿ç”¨ç»å¯¹è·¯å¾„æ£€æŸ¥æœ¬åœ°æ¨¡åž‹
                abs_cache_dir = os.path.abspath(model_cache_dir)
                local_model_path = os.path.join(
                    abs_cache_dir,
                    'models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2'
                )
                
                logger.info(f"ðŸ” æ£€æŸ¥æœ¬åœ°æ¨¡åž‹è·¯å¾„: {local_model_path}")
                logger.info(f"ðŸ” è·¯å¾„å­˜åœ¨æ£€æŸ¥: {os.path.exists(local_model_path)}")
                
                # æ£€æŸ¥å¿«ç…§ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”æœ‰å†…å®¹
                snapshots_dir = os.path.join(local_model_path, 'snapshots')
                has_valid_model = False
                if os.path.exists(snapshots_dir):
                    try:
                        snapshots = os.listdir(snapshots_dir)
                        if snapshots:
                            logger.info(f"âœ… å‘çŽ°æœ¬åœ°æ¨¡åž‹å¿«ç…§: {snapshots}")
                            has_valid_model = True
                    except Exception as e:
                        logger.warning(f"âš ï¸ æ£€æŸ¥å¿«ç…§å¤±è´¥: {e}")
                
                # ä¼˜å…ˆå°è¯•ä»Žæœ¬åœ°è·¯å¾„åŠ è½½
                if has_valid_model:
                    logger.info(f"âœ… æ£€æµ‹åˆ°å®Œæ•´æœ¬åœ°æ¨¡åž‹ï¼Œä½¿ç”¨ç¦»çº¿æ¨¡å¼åŠ è½½")
                    try:
                        self.embedding_model = SentenceTransformer(
                            'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                            cache_folder=abs_cache_dir,
                            device='cpu',
                            trust_remote_code=True,
                            local_files_only=True  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
                        )
                        logger.info("âœ… Embeddingæ¨¡åž‹åŠ è½½æˆåŠŸ (ç¦»çº¿æ¨¡å¼)")
                    except Exception as local_err:
                        logger.warning(f"âš ï¸ ç¦»çº¿æ¨¡å¼åŠ è½½å¤±è´¥: {str(local_err)}")
                        logger.info("ðŸ”„ å°è¯•åœ¨çº¿æ¨¡å¼...")
                        raise local_err
                else:
                    logger.info("ðŸ“¥ æœ¬åœ°æ¨¡åž‹ä¸å®Œæ•´æˆ–ä¸å­˜åœ¨ï¼Œå°†è”ç½‘ä¸‹è½½...")
                    logger.info(f"   ä¸‹è½½åŽå°†ä¿å­˜åˆ°: {abs_cache_dir}")
                    self.embedding_model = SentenceTransformer(
                        'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                        cache_folder=abs_cache_dir,
                        device='cpu',
                        trust_remote_code=True,
                        local_files_only=False  # å…è®¸è”ç½‘ä¸‹è½½
                    )
                    logger.info("âœ… Embeddingæ¨¡åž‹åŠ è½½æˆåŠŸ (åœ¨çº¿ä¸‹è½½)")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½å¤šè¯­è¨€æ¨¡åž‹: {str(e)}")
                logger.error(f"âŒ è¯¦ç»†é”™è¯¯: {repr(e)}")
                import traceback
                logger.error(f"âŒ é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                logger.info("ðŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡åž‹: sentence-transformers/all-MiniLM-L6-v2")
                try:
                    # é™çº§åˆ°æ›´å°çš„æ¨¡åž‹ä½œä¸ºå¤‡é€‰
                    self.embedding_model = SentenceTransformer(
                        'sentence-transformers/all-MiniLM-L6-v2',
                        cache_folder=model_cache_dir,
                        device='cpu',
                        trust_remote_code=False
                    )
                    logger.info("âœ… ä½¿ç”¨å¤‡ç”¨Embeddingæ¨¡åž‹ (all-MiniLM-L6-v2)")
                except Exception as e2:
                    logger.error(f"âŒ æ‰€æœ‰æ¨¡åž‹åŠ è½½å¤±è´¥: {str(e2)}")
                    logger.error(f"âŒ è¯¦ç»†é”™è¯¯: {repr(e2)}")
                    import traceback
                    logger.error(f"âŒ é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                    logger.error("ðŸ’¡ æ¨¡åž‹é¦–æ¬¡ä½¿ç”¨éœ€è¦è”ç½‘ä¸‹è½½ï¼ˆçº¦420MBï¼‰")
                    logger.error("   æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡åž‹æ–‡ä»¶åˆ° embedding ç›®å½•")
                    logger.error(f"ðŸ’¡ æœŸæœ›çš„æ¨¡åž‹ç›®å½•ç»“æž„:")
                    logger.error(f"   {os.path.abspath(model_cache_dir)}/models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2/")
                    raise RuntimeError("æ— æ³•åŠ è½½ä»»ä½•Embeddingæ¨¡åž‹")
            
            self._initialized = True
            logger.info("âœ… MemoryServiceåˆå§‹åŒ–æˆåŠŸ")
            logger.info(f"  - ChromaDBç›®å½•: {chroma_dir}")
            logger.info(f"  - Embeddingæ¨¡åž‹: paraphrase-multilingual-MiniLM-L12-v2")
            
        except Exception as e:
            logger.error(f"âŒ MemoryServiceåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
    
    def get_collection(self, user_id: str, project_id: str):
        """
        èŽ·å–æˆ–åˆ›å»ºé¡¹ç›®çš„è®°å¿†é›†åˆ
        
        æ¯ä¸ªç”¨æˆ·çš„æ¯ä¸ªé¡¹ç›®æœ‰ç‹¬ç«‹çš„collection,å®žçŽ°æ•°æ®éš”ç¦»
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
        
        Returns:
            ChromaDB Collectionå¯¹è±¡
        """
        # ChromaDB collectionå‘½åè§„åˆ™ï¼š
        # 1. 3-63å­—ç¬¦ï¼ˆæœ€é‡è¦ï¼ï¼‰
        # 2. å¼€å¤´å’Œç»“å°¾å¿…é¡»æ˜¯å­—æ¯æˆ–æ•°å­—
        # 3. åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿æˆ–çŸ­æ¨ªçº¿
        # 4. ä¸èƒ½åŒ…å«è¿žç»­çš„ç‚¹(..)
        # 5. ä¸èƒ½æ˜¯æœ‰æ•ˆçš„IPv4åœ°å€
        
        # ä½¿ç”¨SHA256å“ˆå¸ŒåŽ‹ç¼©IDé•¿åº¦ï¼Œç¡®ä¿ä¸è¶…è¿‡63å­—ç¬¦
        # æ ¼å¼: u_{user_hash}_p_{project_hash} (çº¦30å­—ç¬¦)
        user_hash = hashlib.sha256(user_id.encode()).hexdigest()[:8]
        project_hash = hashlib.sha256(project_id.encode()).hexdigest()[:8]
        collection_name = f"u_{user_hash}_p_{project_hash}"
        
        try:
            return self.client.get_or_create_collection(
                name=collection_name,
                metadata={
                    "user_id": user_id,
                    "project_id": project_id,
                    "created_at": datetime.now().isoformat()
                }
            )
        except Exception as e:
            logger.error(f"âŒ èŽ·å–collectionå¤±è´¥: {str(e)}")
            raise
    
    async def add_memory(
        self,
        user_id: str,
        project_id: str,
        memory_id: str,
        content: str,
        memory_type: str,
        metadata: Dict[str, Any]
    ) -> bool:
        """
        æ·»åŠ è®°å¿†åˆ°å‘é‡æ•°æ®åº“
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            memory_id: è®°å¿†å”¯ä¸€ID
            content: è®°å¿†å†…å®¹(å°†è¢«è½¬æ¢ä¸ºå‘é‡)
            memory_type: è®°å¿†ç±»åž‹
            metadata: é™„åŠ å…ƒæ•°æ®
        
        Returns:
            æ˜¯å¦æ·»åŠ æˆåŠŸ
        """
        try:
            collection = self.get_collection(user_id, project_id)
            
            # ç”Ÿæˆæ–‡æœ¬çš„å‘é‡è¡¨ç¤º
            embedding = self.embedding_model.encode(content).tolist()
            
            # å‡†å¤‡å…ƒæ•°æ®(ChromaDBè¦æ±‚æ‰€æœ‰å€¼ä¸ºåŸºç¡€ç±»åž‹)
            chroma_metadata = {
                "memory_type": memory_type,
                "chapter_id": str(metadata.get("chapter_id", "")),
                "chapter_number": int(metadata.get("chapter_number", 0)),
                "importance": float(metadata.get("importance_score", 0.5)),
                "tags": json.dumps(metadata.get("tags", []), ensure_ascii=False),
                "title": str(metadata.get("title", ""))[:200],  # é™åˆ¶é•¿åº¦
                "is_foreshadow": int(metadata.get("is_foreshadow", 0)),
                "created_at": datetime.now().isoformat()
            }
            
            # æ·»åŠ ç›¸å…³è§’è‰²ä¿¡æ¯
            if metadata.get("related_characters"):
                chroma_metadata["related_characters"] = json.dumps(
                    metadata["related_characters"], 
                    ensure_ascii=False
                )
            
            # å­˜å‚¨åˆ°å‘é‡åº“
            collection.add(
                ids=[memory_id],
                embeddings=[embedding],
                documents=[content],
                metadatas=[chroma_metadata]
            )
            
            logger.info(f"âœ… è®°å¿†å·²æ·»åŠ : {memory_id[:8]}... (ç±»åž‹:{memory_type}, é‡è¦æ€§:{chroma_metadata['importance']})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def batch_add_memories(
        self,
        user_id: str,
        project_id: str,
        memories: List[Dict[str, Any]]
    ) -> int:
        """
        æ‰¹é‡æ·»åŠ è®°å¿†(æ€§èƒ½æ›´å¥½)
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            memories: è®°å¿†åˆ—è¡¨,æ¯ä¸ªåŒ…å«idã€contentã€typeã€metadata
        
        Returns:
            æˆåŠŸæ·»åŠ çš„æ•°é‡
        """
        if not memories:
            return 0
            
        try:
            collection = self.get_collection(user_id, project_id)
            
            ids = []
            documents = []
            metadatas = []
            embeddings = []
            
            # æ‰¹é‡å‡†å¤‡æ•°æ®
            for mem in memories:
                ids.append(mem['id'])
                documents.append(mem['content'])
                
                # ç”Ÿæˆembedding
                embedding = self.embedding_model.encode(mem['content']).tolist()
                embeddings.append(embedding)
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = mem.get('metadata', {})
                chroma_metadata = {
                    "memory_type": mem['type'],
                    "chapter_id": str(metadata.get("chapter_id", "")),
                    "chapter_number": int(metadata.get("chapter_number", 0)),
                    "importance": float(metadata.get("importance_score", 0.5)),
                    "tags": json.dumps(metadata.get("tags", []), ensure_ascii=False),
                    "title": str(metadata.get("title", ""))[:200],
                    "is_foreshadow": int(metadata.get("is_foreshadow", 0)),
                    "created_at": datetime.now().isoformat()
                }
                metadatas.append(chroma_metadata)
            
            # æ‰¹é‡æ·»åŠ 
            collection.add(
                ids=ids,
                embeddings=embeddings,
                documents=documents,
                metadatas=metadatas
            )
            
            logger.info(f"âœ… æ‰¹é‡æ·»åŠ è®°å¿†æˆåŠŸ: {len(memories)}æ¡")
            return len(memories)
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ·»åŠ è®°å¿†å¤±è´¥: {str(e)}")
            return 0
    
    async def search_memories(
        self,
        user_id: str,
        project_id: str,
        query: str,
        memory_types: Optional[List[str]] = None,
        limit: int = 10,
        min_importance: float = 0.0,
        chapter_range: Optional[tuple] = None
    ) -> List[Dict[str, Any]]:
        """
        è¯­ä¹‰æœç´¢ç›¸å…³è®°å¿†
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            query: æŸ¥è¯¢æ–‡æœ¬(ä¼šè¢«è½¬æ¢ä¸ºå‘é‡è¿›è¡Œç›¸ä¼¼åº¦æœç´¢)
            memory_types: è¿‡æ»¤ç‰¹å®šç±»åž‹çš„è®°å¿†
            limit: è¿”å›žç»“æžœæ•°é‡
            min_importance: æœ€ä½Žé‡è¦æ€§é˜ˆå€¼
            chapter_range: ç« èŠ‚èŒƒå›´ (start, end)
        
        Returns:
            ç›¸å…³è®°å¿†åˆ—è¡¨,æŒ‰ç›¸ä¼¼åº¦æŽ’åº
        """
        try:
            collection = self.get_collection(user_id, project_id)
            
            # ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_embedding = self.embedding_model.encode(query).tolist()
            
            # æž„å»ºè¿‡æ»¤æ¡ä»¶ - ChromaDBè¦æ±‚ä½¿ç”¨$andç»„åˆå¤šä¸ªæ¡ä»¶
            where_filter = None
            conditions = []
            
            if memory_types:
                conditions.append({"memory_type": {"$in": memory_types}})
            if min_importance > 0:
                conditions.append({"importance": {"$gte": min_importance}})
            if chapter_range:
                conditions.append({"chapter_number": {"$gte": chapter_range[0]}})
                conditions.append({"chapter_number": {"$lte": chapter_range[1]}})
            
            # æ ¹æ®æ¡ä»¶æ•°é‡é€‰æ‹©åˆé€‚çš„æ ¼å¼
            if len(conditions) == 0:
                where_filter = None
            elif len(conditions) == 1:
                where_filter = conditions[0]
            else:
                where_filter = {"$and": conditions}
            
            # æ‰§è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=limit,
                where=where_filter
            )
            
            # æ ¼å¼åŒ–ç»“æžœ
            memories = []
            if results['ids'] and results['ids'][0]:
                for i in range(len(results['ids'][0])):
                    memories.append({
                        "id": results['ids'][0][i],
                        "content": results['documents'][0][i],
                        "metadata": results['metadatas'][0][i],
                        "similarity": 1 - results['distances'][0][i] if 'distances' in results else 1.0,
                        "distance": results['distances'][0][i] if 'distances' in results else 0.0
                    })
            
            logger.info(f"ðŸ” è¯­ä¹‰æœç´¢å®Œæˆ: æŸ¥è¯¢='{query[:30]}...', æ‰¾åˆ°{len(memories)}æ¡è®°å¿†")
            return memories
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢è®°å¿†å¤±è´¥: {str(e)}")
            return []
    
    async def get_recent_memories(
        self,
        user_id: str,
        project_id: str,
        current_chapter: int,
        recent_count: int = 3,
        min_importance: float = 0.5
    ) -> List[Dict[str, Any]]:
        """
        èŽ·å–æœ€è¿‘å‡ ç« çš„é‡è¦è®°å¿†(ç”¨äºŽä¿æŒè¿žè´¯æ€§)
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            current_chapter: å½“å‰ç« èŠ‚å·
            recent_count: èŽ·å–æœ€è¿‘å‡ ç« 
            min_importance: æœ€ä½Žé‡è¦æ€§é˜ˆå€¼
        
        Returns:
            æœ€è¿‘ç« èŠ‚çš„è®°å¿†åˆ—è¡¨,æŒ‰é‡è¦æ€§æŽ’åº
        """
        try:
            collection = self.get_collection(user_id, project_id)
            
            # è®¡ç®—ç« èŠ‚èŒƒå›´
            start_chapter = max(1, current_chapter - recent_count)
            
            # èŽ·å–æœ€è¿‘ç« èŠ‚çš„è®°å¿†
            results = collection.get(
                where={
                    "$and": [
                        {"chapter_number": {"$gte": start_chapter}},
                        {"chapter_number": {"$lt": current_chapter}},
                        {"importance": {"$gte": min_importance}}
                    ]
                },
                limit=100  # å…ˆèŽ·å–è¶³å¤Ÿå¤šçš„è®°å¿†
            )
            
            memories = []
            if results['ids']:
                for i in range(len(results['ids'])):
                    memories.append({
                        "id": results['ids'][i],
                        "content": results['documents'][i],
                        "metadata": results['metadatas'][i]
                    })
            
            # æŒ‰é‡è¦æ€§å’Œç« èŠ‚å·æŽ’åº
            memories.sort(
                key=lambda x: (float(x['metadata'].get('importance', 0)), 
                              int(x['metadata'].get('chapter_number', 0))),
                reverse=True
            )
            
            # è¿”å›žæœ€é‡è¦çš„å‰Næ¡
            top_memories = memories[:20]
            logger.info(f"ðŸ“š èŽ·å–æœ€è¿‘è®°å¿†: ç« èŠ‚{start_chapter}-{current_chapter-1}, æ‰¾åˆ°{len(top_memories)}æ¡")
            return top_memories
            
        except Exception as e:
            logger.error(f"âŒ èŽ·å–æœ€è¿‘è®°å¿†å¤±è´¥: {str(e)}")
            return []
    
    async def find_unresolved_foreshadows(
        self,
        user_id: str,
        project_id: str,
        current_chapter: int
    ) -> List[Dict[str, Any]]:
        """
        æŸ¥æ‰¾æœªå®Œç»“çš„ä¼ç¬”
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            current_chapter: å½“å‰ç« èŠ‚å·
        
        Returns:
            æœªå®Œç»“ä¼ç¬”åˆ—è¡¨
        """
        try:
            collection = self.get_collection(user_id, project_id)
            
            # æŸ¥æ‰¾ä¼ç¬”çŠ¶æ€ä¸º1(å·²åŸ‹ä¸‹ä½†æœªå›žæ”¶)çš„è®°å¿†
            results = collection.get(
                where={
                    "$and": [
                        {"is_foreshadow": 1},
                        {"chapter_number": {"$lt": current_chapter}}
                    ]
                },
                limit=50
            )
            
            foreshadows = []
            if results['ids']:
                for i in range(len(results['ids'])):
                    foreshadows.append({
                        "id": results['ids'][i],
                        "content": results['documents'][i],
                        "metadata": results['metadatas'][i]
                    })
            
            # æŒ‰é‡è¦æ€§æŽ’åº
            foreshadows.sort(
                key=lambda x: float(x['metadata'].get('importance', 0)),
                reverse=True
            )
            
            logger.info(f"ðŸŽ£ æ‰¾åˆ°æœªå®Œç»“ä¼ç¬”: {len(foreshadows)}ä¸ª")
            return foreshadows
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥æ‰¾ä¼ç¬”å¤±è´¥: {str(e)}")
            return []
    
    async def build_context_for_generation(
        self,
        user_id: str,
        project_id: str,
        current_chapter: int,
        chapter_outline: str,
        character_names: List[str] = None
    ) -> Dict[str, Any]:
        """
        ä¸ºç« èŠ‚ç”Ÿæˆæž„å»ºæ™ºèƒ½ä¸Šä¸‹æ–‡
        
        è¿™æ˜¯æ ¸å¿ƒåŠŸèƒ½: ç»“åˆå¤šç§æ£€ç´¢ç­–ç•¥,ä¸ºAIç”Ÿæˆæä¾›æœ€ç›¸å…³çš„è®°å¿†
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            current_chapter: å½“å‰ç« èŠ‚å·
            chapter_outline: æœ¬ç« å¤§çº²
            character_names: æ¶‰åŠçš„è§’è‰²ååˆ—è¡¨
        
        Returns:
            åŒ…å«å„ç§ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å­—å…¸
        """
        logger.info(f"ðŸ§  å¼€å§‹æž„å»ºç« èŠ‚{current_chapter}çš„æ™ºèƒ½ä¸Šä¸‹æ–‡...")
        
        # 1. èŽ·å–æœ€è¿‘ç« èŠ‚ä¸Šä¸‹æ–‡(æ—¶é—´è¿žç»­æ€§)
        recent = await self.get_recent_memories(
            user_id, project_id, current_chapter, 
            recent_count=3, min_importance=0.5
        )
        
        # 2. è¯­ä¹‰æœç´¢ç›¸å…³è®°å¿†
        relevant = await self.search_memories(
            user_id=user_id,
            project_id=project_id,
            query=chapter_outline,
            limit=10,
            min_importance=0.4
        )
        
        # 3. æŸ¥æ‰¾æœªå®Œç»“ä¼ç¬”
        foreshadows = await self.find_unresolved_foreshadows(
            user_id, project_id, current_chapter
        )
        
        # 4. å¦‚æžœæœ‰æŒ‡å®šè§’è‰²,èŽ·å–è§’è‰²ç›¸å…³è®°å¿†
        character_memories = []
        if character_names:
            character_query = " ".join(character_names) + " è§’è‰² çŠ¶æ€ å…³ç³»"
            character_memories = await self.search_memories(
                user_id=user_id,
                project_id=project_id,
                query=character_query,
                memory_types=["character_event", "plot_point"],
                limit=8
            )
        
        # 5. èŽ·å–é‡è¦æƒ…èŠ‚ç‚¹
        # æ³¨æ„ï¼šChromaDBçš„whereæ¡ä»¶éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œä¸èƒ½åŒæ—¶ä½¿ç”¨å¤šä¸ªé¡¶å±‚æ¡ä»¶
        try:
            plot_points = await self.search_memories(
                user_id=user_id,
                project_id=project_id,
                query="é‡è¦ è½¬æŠ˜ é«˜æ½® å…³é”®",
                memory_types=["plot_point", "hook"],
                limit=5,
                min_importance=0.7
            )
        except Exception as e:
            logger.error(f"âŒ æœç´¢è®°å¿†å¤±è´¥: {str(e)}")
            # é™çº§å¤„ç†ï¼šåˆ†åˆ«æŸ¥è¯¢
            plot_points = []
            try:
                plot_points = await self.search_memories(
                    user_id=user_id,
                    project_id=project_id,
                    query="é‡è¦ è½¬æŠ˜ é«˜æ½® å…³é”®",
                    memory_types=["plot_point", "hook"],
                    limit=5
                )
            except Exception as e2:
                logger.warning(f"âš ï¸ é™çº§æŸ¥è¯¢ä¹Ÿå¤±è´¥: {str(e2)}")
                plot_points = []
        
        context = {
            "recent_context": self._format_memories(recent, "æœ€è¿‘ç« èŠ‚è®°å¿†"),
            "relevant_memories": self._format_memories(relevant, "è¯­ä¹‰ç›¸å…³è®°å¿†"),
            "character_states": self._format_memories(character_memories, "è§’è‰²ç›¸å…³è®°å¿†"),
            "foreshadows": self._format_memories(foreshadows[:5], "æœªå®Œç»“ä¼ç¬”"),
            "plot_points": self._format_memories(plot_points, "é‡è¦æƒ…èŠ‚ç‚¹"),
            "stats": {
                "recent_count": len(recent),
                "relevant_count": len(relevant),
                "character_count": len(character_memories),
                "foreshadow_count": len(foreshadows),
                "plot_point_count": len(plot_points)
            }
        }
        
        logger.info(f"âœ… ä¸Šä¸‹æ–‡æž„å»ºå®Œæˆ: æœ€è¿‘{len(recent)}æ¡, ç›¸å…³{len(relevant)}æ¡, ä¼ç¬”{len(foreshadows)}ä¸ª")
        return context
    def _format_memories(self, memories: List[Dict], section_title: str = "è®°å¿†") -> str:
        """
        æ ¼å¼åŒ–è®°å¿†åˆ—è¡¨ä¸ºæ–‡æœ¬
        
        Args:
            memories: è®°å¿†åˆ—è¡¨
            section_title: ç« èŠ‚æ ‡é¢˜
        
        Returns:
            æ ¼å¼åŒ–åŽçš„æ–‡æœ¬
        """
        if not memories:
            return f"ã€{section_title}ã€‘\næš‚æ— ç›¸å…³è®°å¿†\n"
        
        lines = [f"ã€{section_title}ã€‘"]
        for i, mem in enumerate(memories, 1):
            meta = mem.get('metadata', {})
            chapter_num = meta.get('chapter_number', '?')
            mem_type = meta.get('memory_type', 'æœªçŸ¥')
            importance = float(meta.get('importance', 0.5))
            title = meta.get('title', '')
            content = mem['content']
            
            # æ ¼å¼: [åºå·] ç¬¬Xç« -ç±»åž‹(é‡è¦æ€§) æ ‡é¢˜: å†…å®¹
            line = f"{i}. [ç¬¬{chapter_num}ç« -{mem_type}â˜…{importance:.1f}]"
            if title:
                line += f" {title}: {content[:100]}"
            else:
                line += f" {content[:150]}"
            lines.append(line)
        
        return "\n".join(lines) + "\n"
    
    async def delete_chapter_memories(
        self,
        user_id: str,
        project_id: str,
        chapter_id: str
    ) -> bool:
        """
        åˆ é™¤æŒ‡å®šç« èŠ‚çš„æ‰€æœ‰è®°å¿†
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            chapter_id: ç« èŠ‚ID
        
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            collection = self.get_collection(user_id, project_id)
            
            # æŸ¥æ‰¾è¯¥ç« èŠ‚çš„æ‰€æœ‰è®°å¿†
            results = collection.get(
                where={"chapter_id": chapter_id}
            )
            
            if results['ids']:
                # åˆ é™¤è¿™äº›è®°å¿†
                collection.delete(ids=results['ids'])
                logger.info(f"ðŸ—‘ï¸ å·²åˆ é™¤ç« èŠ‚{chapter_id[:8]}çš„{len(results['ids'])}æ¡è®°å¿†")
                return True
            else:
                logger.info(f"â„¹ï¸ ç« èŠ‚{chapter_id[:8]}æ²¡æœ‰è®°å¿†éœ€è¦åˆ é™¤")
                return True
                
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤ç« èŠ‚è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def delete_project_memories(
        self,
        user_id: str,
        project_id: str
    ) -> bool:
        """
        åˆ é™¤æŒ‡å®šé¡¹ç›®çš„æ‰€æœ‰è®°å¿†(åŒ…æ‹¬å‘é‡æ•°æ®åº“)
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
        
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            # ç”Ÿæˆcollectionåç§°
            user_hash = hashlib.sha256(user_id.encode()).hexdigest()[:8]
            project_hash = hashlib.sha256(project_id.encode()).hexdigest()[:8]
            collection_name = f"u_{user_hash}_p_{project_hash}"
            
            # åˆ é™¤æ•´ä¸ªcollection(è¿™ä¼šæ¸…ç†æ‰€æœ‰å‘é‡æ•°æ®)
            try:
                self.client.delete_collection(name=collection_name)
                logger.info(f"ðŸ—‘ï¸ å·²åˆ é™¤é¡¹ç›®{project_id[:8]}çš„å‘é‡æ•°æ®åº“collection: {collection_name}")
                return True
            except Exception as e:
                # å¦‚æžœcollectionä¸å­˜åœ¨,ä¹Ÿç®—æˆåŠŸ
                if "does not exist" in str(e).lower():
                    logger.info(f"â„¹ï¸ é¡¹ç›®{project_id[:8]}çš„collectionä¸å­˜åœ¨,æ— éœ€åˆ é™¤")
                    return True
                else:
                    raise
                
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤é¡¹ç›®è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def update_memory(
        self,
        user_id: str,
        project_id: str,
        memory_id: str,
        content: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        æ›´æ–°è®°å¿†å†…å®¹æˆ–å…ƒæ•°æ®
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            memory_id: è®°å¿†ID
            content: æ–°å†…å®¹(å¯é€‰)
            metadata: æ–°å…ƒæ•°æ®(å¯é€‰)
        
        Returns:
            æ˜¯å¦æ›´æ–°æˆåŠŸ
        """
        try:
            collection = self.get_collection(user_id, project_id)
            
            update_data = {}
            
            if content:
                # é‡æ–°ç”Ÿæˆembedding
                embedding = self.embedding_model.encode(content).tolist()
                update_data['embeddings'] = [embedding]
                update_data['documents'] = [content]
            
            if metadata:
                # å‡†å¤‡æ–°çš„å…ƒæ•°æ®
                chroma_metadata = {}
                for key, value in metadata.items():
                    if isinstance(value, (list, dict)):
                        chroma_metadata[key] = json.dumps(value, ensure_ascii=False)
                    else:
                        chroma_metadata[key] = value
                update_data['metadatas'] = [chroma_metadata]
            
            if update_data:
                collection.update(
                    ids=[memory_id],
                    **update_data
                )
                logger.info(f"âœ… è®°å¿†å·²æ›´æ–°: {memory_id[:8]}...")
                return True
            else:
                logger.warning("âš ï¸ æ²¡æœ‰æä¾›æ›´æ–°å†…å®¹")
                return False
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°è®°å¿†å¤±è´¥: {str(e)}")
            return False
    
    async def get_memory_stats(
        self,
        user_id: str,
        project_id: str
    ) -> Dict[str, Any]:
        """
        èŽ·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            collection = self.get_collection(user_id, project_id)
            
            # èŽ·å–æ‰€æœ‰è®°å¿†
            all_memories = collection.get()
            
            if not all_memories['ids']:
                return {
                    "total_count": 0,
                    "by_type": {},
                    "by_chapter": {},
                    "foreshadow_count": 0
                }
            
            # ç»Ÿè®¡å„ç±»åž‹æ•°é‡
            type_counts = {}
            chapter_counts = {}
            foreshadow_count = 0
            
            for i, meta in enumerate(all_memories['metadatas']):
                mem_type = meta.get('memory_type', 'unknown')
                chapter_num = meta.get('chapter_number', 0)
                is_foreshadow = meta.get('is_foreshadow', 0)
                
                type_counts[mem_type] = type_counts.get(mem_type, 0) + 1
                chapter_counts[str(chapter_num)] = chapter_counts.get(str(chapter_num), 0) + 1
                
                if is_foreshadow == 1:
                    foreshadow_count += 1
            
            stats = {
                "total_count": len(all_memories['ids']),
                "by_type": type_counts,
                "by_chapter": chapter_counts,
                "foreshadow_count": foreshadow_count,
                "foreshadow_resolved": sum(1 for m in all_memories['metadatas'] if m.get('is_foreshadow') == 2)
            }
            
            logger.info(f"ðŸ“Š è®°å¿†ç»Ÿè®¡: æ€»è®¡{stats['total_count']}æ¡, ä¼ç¬”{stats['foreshadow_count']}ä¸ª")
            return stats
            
        except Exception as e:
            logger.error(f"âŒ èŽ·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {"error": str(e)}

    async def run_state_cleanup(
        self,
        user_id: str,
        project_id: str,
        start_chapter: int,
        end_chapter: int,
        db: AsyncSession,
        ai_service: Any
    ) -> Dict[str, Any]:
        """
        è¿è¡ŒçŠ¶æ€æ¸…ç†/ç»´æŠ¤ (State Manager)
        
        æ¯éš”ä¸€å®šç« èŠ‚(å¦‚10ç« )è¿è¡Œä¸€æ¬¡ï¼Œç”¨äºŽï¼š
        1. æ ‡è®°æ­»äº¡è§’è‰²
        2. æ›´æ–°è§’è‰²å…³ç³»
        3. å½’æ¡£å·²æ¶ˆè€—ç‰©å“
        4. æ ‡è®°å·²å›žæ”¶ä¼ç¬”
        
        Args:
            user_id: ç”¨æˆ·ID
            project_id: é¡¹ç›®ID
            start_chapter: å¼€å§‹ç« èŠ‚
            end_chapter: ç»“æŸç« èŠ‚
            db: æ•°æ®åº“ä¼šè¯
            ai_service: AIæœåŠ¡å®žä¾‹
            
        Returns:
            ç»´æŠ¤æŠ¥å‘Š
        """
        logger.info(f"ðŸ§¹ å¼€å§‹è¿è¡ŒçŠ¶æ€ç»´æŠ¤: {project_id} (Ch {start_chapter}-{end_chapter})")
        report = {"updates": [], "errors": []}
        
        try:
            # 1. èŽ·å–å½“å‰çŠ¶æ€ä¸Šä¸‹æ–‡
            # 1.1 èŽ·å–æ´»è·ƒè§’è‰²
            char_result = await db.execute(
                select(Character).where(Character.project_id == project_id)
            )
            characters = char_result.scalars().all()
            
            active_chars = []
            char_map = {}
            for c in characters:
                char_map[c.name] = c
                # æ£€æŸ¥æ˜¯å¦å·²æ­»äº¡
                traits = json.loads(c.traits) if c.traits else []
                status = "ACTIVE"
                if "DEAD" in traits or "STATUS:DEAD" in traits:
                    status = "DEAD"
                
                if status == "ACTIVE":
                    active_chars.append({
                        "name": c.name,
                        "role": c.role_type,
                        "status": status
                    })
            
            # 1.2 èŽ·å–æœªå›žæ”¶ä¼ç¬”
            foreshadow_result = await db.execute(
                select(StoryMemory)
                .where(StoryMemory.project_id == project_id)
                .where(StoryMemory.is_foreshadow == 1)
            )
            foreshadows = foreshadow_result.scalars().all()
            active_foreshadows = [
                {"id": f.id, "content": f.content, "chapter": f.story_timeline}
                for f in foreshadows
            ]
            
            # 1.3 æž„å»ºå½“å‰çŠ¶æ€JSON
            current_state = {
                "active_characters": [c["name"] for c in active_chars],
                "unresolved_foreshadows": active_foreshadows
            }
            
            # 2. èŽ·å–å‰§æƒ…æ‘˜è¦ (ä»ŽPlotAnalysisèŽ·å–)
            plot_result = await db.execute(
                select(PlotAnalysis)
                .where(PlotAnalysis.project_id == project_id)
                .where(PlotAnalysis.chapter_id.in_(
                    select(StoryMemory.chapter_id) # è¿™é‡Œçš„å…³è”æœ‰ç‚¹ç»•ï¼Œæˆ‘ä»¬ç›´æŽ¥æŸ¥ç« èŠ‚å·
                    # ç®€åŒ–ï¼šç›´æŽ¥æŸ¥è¯¢PlotAnalysisï¼Œå‡è®¾æˆ‘ä»¬èƒ½é€šè¿‡chapter_numberå…³è”
                    # ä½†PlotAnalysisåªæœ‰chapter_idã€‚æˆ‘ä»¬éœ€è¦å…ˆæŸ¥ç« èŠ‚IDã€‚
                ))
            )
            
            # æ›´ç®€å•çš„æ–¹æ³•ï¼šç›´æŽ¥æŸ¥è¯¢èŒƒå›´å†…çš„Chaptersï¼Œç„¶åŽæŸ¥PlotAnalysis
            # æˆ–è€…ç›´æŽ¥æŸ¥è¯¢StoryMemoryä¸­çš„chapter_summary
            
            # å°è¯•ä»ŽStoryMemoryèŽ·å–chapter_summary
            summary_result = await db.execute(
                select(StoryMemory)
                .where(StoryMemory.project_id == project_id)
                .where(StoryMemory.story_timeline >= start_chapter)
                .where(StoryMemory.story_timeline <= end_chapter)
                .where(StoryMemory.memory_type == "chapter_summary")
                .order_by(StoryMemory.story_timeline)
            )
            summaries = summary_result.scalars().all()
            
            recent_plot = "\n".join([
                f"ç¬¬{s.story_timeline}ç« : {s.content}" for s in summaries
            ])
            
            if not recent_plot:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°å‰§æƒ…æ‘˜è¦ï¼Œè·³è¿‡çŠ¶æ€ç»´æŠ¤")
                return report
                
            # 3. è°ƒç”¨AI
            prompt = PromptService.STATE_MANAGER.format(
                start_chapter=start_chapter,
                end_chapter=end_chapter,
                current_state_json=json.dumps(current_state, ensure_ascii=False, indent=2),
                recent_plot_summary=recent_plot
            )
            
            response = await ai_service.generate_text(
                prompt=prompt,
                auto_mcp=False, # ä¸éœ€è¦MCPï¼Œçº¯é€»è¾‘å¤„ç†
                temperature=0.3 # ä½Žæ¸©åº¦ï¼Œä¿è¯é€»è¾‘æ€§
            )
            
            # 4. è§£æžç»“æžœå¹¶æ›´æ–°
            try:
                result_json = parse_json(response.get("content", "{}"))
            except Exception as e:
                logger.error(f"âŒ è§£æžçŠ¶æ€ç®¡ç†å“åº”å¤±è´¥: {e}")
                return report
                
            # 5. æ‰§è¡Œæ›´æ–°
            # 5.1 å¤„ç†æ­»äº¡è§’è‰²
            dead_chars = result_json.get("dead_characters", [])
            # å…¼å®¹æ ¼å¼ï¼šæœ‰äº›æ¨¡åž‹å¯èƒ½è¿”å›ž "characters": [{"name": "X", "status": "DEAD"}]
            if not dead_chars and "characters" in result_json:
                dead_chars = [
                    c["name"] for c in result_json["characters"] 
                    if c.get("status") == "DEAD"
                ]
                
            for char_name in dead_chars:
                if char_name in char_map:
                    char = char_map[char_name]
                    traits = json.loads(char.traits) if char.traits else []
                    if "STATUS:DEAD" not in traits:
                        traits.append("STATUS:DEAD")
                        char.traits = json.dumps(traits, ensure_ascii=False)
                        report["updates"].append(f"è§’è‰² {char_name} æ ‡è®°ä¸ºæ­»äº¡")
            
            # 5.2 å¤„ç†ä¼ç¬”å›žæ”¶
            resolved_foreshadows = result_json.get("resolved_foreshadows", []) # idåˆ—è¡¨
            # å…¼å®¹æ ¼å¼ï¼šå¯èƒ½è¿”å›žå†…å®¹åŒ¹é…
            
            # å¦‚æžœAIè¿”å›žçš„æ˜¯ID (æˆ‘ä»¬ä¼ å…¥äº†ID)
            for f_id in resolved_foreshadows:
                # æŸ¥æ‰¾å¯¹åº”çš„StoryMemory
                memory = next((f for f in foreshadows if f.id == f_id), None)
                if memory:
                    memory.is_foreshadow = 2 # Resolved
                    report["updates"].append(f"ä¼ç¬”å›žæ”¶: {memory.content[:20]}...")
            
            # å¦‚æžœAIè¿”å›žçš„æ˜¯å†…å®¹æè¿°ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é… (TODO)
            
            await db.commit()
            logger.info(f"âœ… çŠ¶æ€ç»´æŠ¤å®Œæˆ: {len(report['updates'])} é¡¹æ›´æ–°")
            
        except Exception as e:
            logger.error(f"âŒ çŠ¶æ€ç»´æŠ¤å¤±è´¥: {str(e)}", exc_info=True)
            report["errors"].append(str(e))
            
        return report


# åˆ›å»ºå…¨å±€å®žä¾‹
memory_service = MemoryService()
            